{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is OS\n",
      " Volume Serial Number is 1C15-3769\n",
      "\n",
      " Directory of c:\\Users\\nrajyaguru\\python files\\Deepseek_Test\n",
      "\n",
      "26-Jan-25  12:13 PM    <DIR>          .\n",
      "26-Jan-25  12:13 PM    <DIR>          ..\n",
      "26-Jan-25  12:11 PM                 0 Netflix_movies.ipynb\n",
      "26-Jan-25  12:13 PM         3,399,671 netflix_titles.csv\n",
      "               2 File(s)      3,399,671 bytes\n",
      "               2 Dir(s)  41,820,897,280 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntm = pd.read_csv(\"netflix_titles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>show_id</th>\n",
       "      <th>type</th>\n",
       "      <th>title</th>\n",
       "      <th>director</th>\n",
       "      <th>cast</th>\n",
       "      <th>country</th>\n",
       "      <th>date_added</th>\n",
       "      <th>release_year</th>\n",
       "      <th>rating</th>\n",
       "      <th>duration</th>\n",
       "      <th>listed_in</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s1</td>\n",
       "      <td>Movie</td>\n",
       "      <td>Dick Johnson Is Dead</td>\n",
       "      <td>Kirsten Johnson</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>September 25, 2021</td>\n",
       "      <td>2020</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>90 min</td>\n",
       "      <td>Documentaries</td>\n",
       "      <td>As her father nears the end of his life, filmm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s2</td>\n",
       "      <td>TV Show</td>\n",
       "      <td>Blood &amp; Water</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ama Qamata, Khosi Ngema, Gail Mabalane, Thaban...</td>\n",
       "      <td>South Africa</td>\n",
       "      <td>September 24, 2021</td>\n",
       "      <td>2021</td>\n",
       "      <td>TV-MA</td>\n",
       "      <td>2 Seasons</td>\n",
       "      <td>International TV Shows, TV Dramas, TV Mysteries</td>\n",
       "      <td>After crossing paths at a party, a Cape Town t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s3</td>\n",
       "      <td>TV Show</td>\n",
       "      <td>Ganglands</td>\n",
       "      <td>Julien Leclercq</td>\n",
       "      <td>Sami Bouajila, Tracy Gotoas, Samuel Jouy, Nabi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>September 24, 2021</td>\n",
       "      <td>2021</td>\n",
       "      <td>TV-MA</td>\n",
       "      <td>1 Season</td>\n",
       "      <td>Crime TV Shows, International TV Shows, TV Act...</td>\n",
       "      <td>To protect his family from a powerful drug lor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s4</td>\n",
       "      <td>TV Show</td>\n",
       "      <td>Jailbirds New Orleans</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>September 24, 2021</td>\n",
       "      <td>2021</td>\n",
       "      <td>TV-MA</td>\n",
       "      <td>1 Season</td>\n",
       "      <td>Docuseries, Reality TV</td>\n",
       "      <td>Feuds, flirtations and toilet talk go down amo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  show_id     type                  title         director  \\\n",
       "0      s1    Movie   Dick Johnson Is Dead  Kirsten Johnson   \n",
       "1      s2  TV Show          Blood & Water              NaN   \n",
       "2      s3  TV Show              Ganglands  Julien Leclercq   \n",
       "3      s4  TV Show  Jailbirds New Orleans              NaN   \n",
       "\n",
       "                                                cast        country  \\\n",
       "0                                                NaN  United States   \n",
       "1  Ama Qamata, Khosi Ngema, Gail Mabalane, Thaban...   South Africa   \n",
       "2  Sami Bouajila, Tracy Gotoas, Samuel Jouy, Nabi...            NaN   \n",
       "3                                                NaN            NaN   \n",
       "\n",
       "           date_added  release_year rating   duration  \\\n",
       "0  September 25, 2021          2020  PG-13     90 min   \n",
       "1  September 24, 2021          2021  TV-MA  2 Seasons   \n",
       "2  September 24, 2021          2021  TV-MA   1 Season   \n",
       "3  September 24, 2021          2021  TV-MA   1 Season   \n",
       "\n",
       "                                           listed_in  \\\n",
       "0                                      Documentaries   \n",
       "1    International TV Shows, TV Dramas, TV Mysteries   \n",
       "2  Crime TV Shows, International TV Shows, TV Act...   \n",
       "3                             Docuseries, Reality TV   \n",
       "\n",
       "                                         description  \n",
       "0  As her father nears the end of his life, filmm...  \n",
       "1  After crossing paths at a party, a Cape Town t...  \n",
       "2  To protect his family from a powerful drug lor...  \n",
       "3  Feuds, flirtations and toilet talk go down amo...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ntm.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8807 entries, 0 to 8806\n",
      "Data columns (total 12 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   show_id       8807 non-null   object\n",
      " 1   type          8807 non-null   object\n",
      " 2   title         8807 non-null   object\n",
      " 3   director      6173 non-null   object\n",
      " 4   cast          7982 non-null   object\n",
      " 5   country       7976 non-null   object\n",
      " 6   date_added    8797 non-null   object\n",
      " 7   release_year  8807 non-null   int64 \n",
      " 8   rating        8803 non-null   object\n",
      " 9   duration      8804 non-null   object\n",
      " 10  listed_in     8807 non-null   object\n",
      " 11  description   8807 non-null   object\n",
      "dtypes: int64(1), object(11)\n",
      "memory usage: 825.8+ KB\n"
     ]
    }
   ],
   "source": [
    "ntm.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'models': [{'name': 'deepseek-r1:8b',\n",
       "   'model': 'deepseek-r1:8b',\n",
       "   'modified_at': '2025-01-25T12:08:16.6195934+05:30',\n",
       "   'size': 4920738407,\n",
       "   'digest': '28f8fd6cdc677661426adab9338ce3c013d7e69a5bea9e704b364171a5d61a10',\n",
       "   'details': {'parent_model': '',\n",
       "    'format': 'gguf',\n",
       "    'family': 'llama',\n",
       "    'families': ['llama'],\n",
       "    'parameter_size': '8.0B',\n",
       "    'quantization_level': 'Q4_K_M'}},\n",
       "  {'name': 'llama3.1:latest',\n",
       "   'model': 'llama3.1:latest',\n",
       "   'modified_at': '2024-08-29T14:52:54.5864813+05:30',\n",
       "   'size': 4661230720,\n",
       "   'digest': 'f66fc8dc39ea206e03ff6764fcc696b1b4dfb693f0b6ef751731dd4e6269046e',\n",
       "   'details': {'parent_model': '',\n",
       "    'format': 'gguf',\n",
       "    'family': 'llama',\n",
       "    'families': ['llama'],\n",
       "    'parameter_size': '8.0B',\n",
       "    'quantization_level': 'Q4_0'}}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama\n",
    "ollama.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'deepseek-r1:8b', 'created_at': '2025-01-26T08:20:03.8373825Z', 'message': {'role': 'assistant', 'content': \"<think>\\nOkay, so I'm trying to figure out why the sky is blue. I've heard this question before, but I don't remember exactly how it's answered. Let me think through this step by step.\\n\\nFirst, I know that when you look up during the day, the sky appears blue. But when the sun sets or the moon rises, the sky can turn different colors like red or purple. That makes me wonder why the color changes and what causes the sky to be blue most of the time.\\n\\nI remember something about Rayleigh scattering from my science classes. I think that's related to how light scatters in the atmosphere. So maybe it has to do with the different wavelengths of light being scattered more or less by the air molecules and particles in the sky.\\n\\nWait, what are the different colors of the spectrum? There's red, orange, yellow, green, blue, indigo, and violet. Each color has a different wavelength. I think blue light has a shorter wavelength compared to other visible colors. So maybe the Earth's atmosphere scatters certain wavelengths more than others.\\n\\nBut why does it scatter blue light more? Is it because of the size of the molecules or something else? I remember that Rayleigh scattering depends on the wavelength of the light and the size of the particles causing the scattering. Maybe the smaller the particle, the more it scatters light with longer wavelengths, but I'm not sure.\\n\\nAnother thought: when sunlight first hits the Earth's atmosphere, it travels through the air. As it does so, some of its colors are scattered away from the direct path. The blue color isn't absorbed as much and instead scatters more. So we see blue because that light is scattered in all directions towards our eyes.\\n\\nBut then why don't we see violet? Because violet has a shorter wavelength than blue, maybe it's scattered even more. Or perhaps violet light is scattered out of our line of sight faster than blue light, making the sky appear blue instead of violet.\\n\\nAlso, during sunrise or sunset, the sun is lower on the horizon, so the light has to pass through more atmosphere before reaching us. That extra scattering probably causes the red hues because shorter wavelengths (blue and violet) scatter more, leaving the longer wavelengths like orange and red to dominate our view.\\n\\nWait, I'm confusing myself a bit. Let me try to structure this:\\n\\n1. The sky appears blue during the day.\\n2. At sunrise or sunset, it turns red or purple.\\n3. This has to do with how sunlight scatters in the atmosphere.\\n\\nSo, Rayleigh scattering must be the key. It explains both the blue daytime sky and the colorful sunsets. For blue light, since it's scattered more by the atmosphere, we see that color. For other colors like red, their wavelengths are longer, so they don't scatter as much. But when sunlight travels through a lot of atmosphere (like during sunrise), the shorter blue and violet wavelengths scatter out of our line of sight, leaving the longer red light to dominate, making sunsets appear red.\\n\\nI also recall that this effect is stronger near the horizon because there's more atmosphere for the light to pass through. That explains why both the sky and the setting sun have different colors depending on how much atmosphere the light passes through.\\n\\nWait, but why isn't the sky violet? Since violet has a shorter wavelength than blue, wouldn't it scatter even more? Maybe violet is scattered out of our line of sight even more quickly, so we don't see as much of it. That's why the sky appears blue rather than violet.\\n\\nSo putting this all together: during the day, when looking straight up, sunlight has traveled through less atmosphere and the shorter blue wavelengths scatter in all directions, making the sky appear blue to us. At sunrise or sunset, the light has gone through more atmosphere, scattering away more of the shorter wavelengths (blue and violet), so we see the longer red wavelengths instead.\\n\\nI think that's a reasonable explanation, but I might be mixing up some details. Maybe I should also consider other factors like ozone in the upper atmosphere, which absorbs certain ultraviolet rays, allowing more blue light to reach us. That would reinforce why blue is dominant without interference from UV absorption.\\n\\nIn summary, the sky is blue because the Earth's atmosphere scatters shorter wavelengths of light (blue) more effectively than longer ones, and this scattering is more pronounced as light passes through a greater amount of atmosphere near the horizon.\\n</think>\\n\\nThe sky appears blue during the day due to the phenomenon known as Rayleigh scattering. This occurs when sunlight travels through the Earth's atmosphere, where shorter wavelengths, such as blue light, are scattered in all directions. The longer wavelengths, like red and orange, pass through more easily. \\n\\nAt sunrise or sunset, the sun is lower on the horizon, meaning sunlight passes through a greater thickness of the atmosphere. Here, shorter wavelengths like blue and violet scatter out of our line of sight more efficiently, leaving the longer red wavelengths to dominate our view. This shift in color explains why sunsets appear red or purple.\\n\\nAdditionally, ozone in the upper atmosphere absorbs certain ultraviolet rays, allowing more blue light to reach us without significant absorption. Thus, the sky appears blue because shorter blue wavelengths scatter effectively, while other colors are either scattered away or absorbed, depending on the path of the light through the atmosphere.\"}, 'done_reason': 'stop', 'done': True, 'total_duration': 376196908400, 'load_duration': 40326300, 'prompt_eval_count': 9, 'prompt_eval_duration': 5000000, 'eval_count': 1089, 'eval_duration': 376149000000}\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "response = ollama.chat(model='deepseek-r1:8b', messages=[{'role': 'user', 'content': 'Why is the sky blue?'}])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'deepseek-r1:8b',\n",
       " 'created_at': '2025-01-26T08:20:03.8373825Z',\n",
       " 'message': {'role': 'assistant',\n",
       "  'content': \"<think>\\nOkay, so I'm trying to figure out why the sky is blue. I've heard this question before, but I don't remember exactly how it's answered. Let me think through this step by step.\\n\\nFirst, I know that when you look up during the day, the sky appears blue. But when the sun sets or the moon rises, the sky can turn different colors like red or purple. That makes me wonder why the color changes and what causes the sky to be blue most of the time.\\n\\nI remember something about Rayleigh scattering from my science classes. I think that's related to how light scatters in the atmosphere. So maybe it has to do with the different wavelengths of light being scattered more or less by the air molecules and particles in the sky.\\n\\nWait, what are the different colors of the spectrum? There's red, orange, yellow, green, blue, indigo, and violet. Each color has a different wavelength. I think blue light has a shorter wavelength compared to other visible colors. So maybe the Earth's atmosphere scatters certain wavelengths more than others.\\n\\nBut why does it scatter blue light more? Is it because of the size of the molecules or something else? I remember that Rayleigh scattering depends on the wavelength of the light and the size of the particles causing the scattering. Maybe the smaller the particle, the more it scatters light with longer wavelengths, but I'm not sure.\\n\\nAnother thought: when sunlight first hits the Earth's atmosphere, it travels through the air. As it does so, some of its colors are scattered away from the direct path. The blue color isn't absorbed as much and instead scatters more. So we see blue because that light is scattered in all directions towards our eyes.\\n\\nBut then why don't we see violet? Because violet has a shorter wavelength than blue, maybe it's scattered even more. Or perhaps violet light is scattered out of our line of sight faster than blue light, making the sky appear blue instead of violet.\\n\\nAlso, during sunrise or sunset, the sun is lower on the horizon, so the light has to pass through more atmosphere before reaching us. That extra scattering probably causes the red hues because shorter wavelengths (blue and violet) scatter more, leaving the longer wavelengths like orange and red to dominate our view.\\n\\nWait, I'm confusing myself a bit. Let me try to structure this:\\n\\n1. The sky appears blue during the day.\\n2. At sunrise or sunset, it turns red or purple.\\n3. This has to do with how sunlight scatters in the atmosphere.\\n\\nSo, Rayleigh scattering must be the key. It explains both the blue daytime sky and the colorful sunsets. For blue light, since it's scattered more by the atmosphere, we see that color. For other colors like red, their wavelengths are longer, so they don't scatter as much. But when sunlight travels through a lot of atmosphere (like during sunrise), the shorter blue and violet wavelengths scatter out of our line of sight, leaving the longer red light to dominate, making sunsets appear red.\\n\\nI also recall that this effect is stronger near the horizon because there's more atmosphere for the light to pass through. That explains why both the sky and the setting sun have different colors depending on how much atmosphere the light passes through.\\n\\nWait, but why isn't the sky violet? Since violet has a shorter wavelength than blue, wouldn't it scatter even more? Maybe violet is scattered out of our line of sight even more quickly, so we don't see as much of it. That's why the sky appears blue rather than violet.\\n\\nSo putting this all together: during the day, when looking straight up, sunlight has traveled through less atmosphere and the shorter blue wavelengths scatter in all directions, making the sky appear blue to us. At sunrise or sunset, the light has gone through more atmosphere, scattering away more of the shorter wavelengths (blue and violet), so we see the longer red wavelengths instead.\\n\\nI think that's a reasonable explanation, but I might be mixing up some details. Maybe I should also consider other factors like ozone in the upper atmosphere, which absorbs certain ultraviolet rays, allowing more blue light to reach us. That would reinforce why blue is dominant without interference from UV absorption.\\n\\nIn summary, the sky is blue because the Earth's atmosphere scatters shorter wavelengths of light (blue) more effectively than longer ones, and this scattering is more pronounced as light passes through a greater amount of atmosphere near the horizon.\\n</think>\\n\\nThe sky appears blue during the day due to the phenomenon known as Rayleigh scattering. This occurs when sunlight travels through the Earth's atmosphere, where shorter wavelengths, such as blue light, are scattered in all directions. The longer wavelengths, like red and orange, pass through more easily. \\n\\nAt sunrise or sunset, the sun is lower on the horizon, meaning sunlight passes through a greater thickness of the atmosphere. Here, shorter wavelengths like blue and violet scatter out of our line of sight more efficiently, leaving the longer red wavelengths to dominate our view. This shift in color explains why sunsets appear red or purple.\\n\\nAdditionally, ozone in the upper atmosphere absorbs certain ultraviolet rays, allowing more blue light to reach us without significant absorption. Thus, the sky appears blue because shorter blue wavelengths scatter effectively, while other colors are either scattered away or absorbed, depending on the path of the light through the atmosphere.\"},\n",
       " 'done_reason': 'stop',\n",
       " 'done': True,\n",
       " 'total_duration': 376196908400,\n",
       " 'load_duration': 40326300,\n",
       " 'prompt_eval_count': 9,\n",
       " 'prompt_eval_duration': 5000000,\n",
       " 'eval_count': 1089,\n",
       " 'eval_duration': 376149000000}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "model= OllamaLLM(model = \"deepseek-r1:8b\"\n",
    ")\n",
    "p = model.invoke(\"What is your name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Greetings! I'm DeepSeek-R1, an artificial intelligence assistant created by DeepSeek. I'm at your service and would be delighted to assist you with any inquiries or tasks you may have.\n",
      "</think>\n",
      "\n",
      "Greetings! I'm DeepSeek-R1, an artificial intelligence assistant created by DeepSeek. I'm at your service and would be delighted to assist you with any inquiries or tasks you may have.\n"
     ]
    }
   ],
   "source": [
    "print (p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I'm trying to figure out how to modify the Python code provided by the user. The goal is to prevent certain parameters related to the DeepSeek-R1 model from being included in the responses generated by an LLM using LangChain and Ollama.\n",
      "\n",
      "First, I need to understand what the original code does. It seems like it's using LangChain with Ollama for generating responses. The user wants to exclude specific parameters from these responses, which are probably related to the model's version or other internal details they don't want revealed.\n",
      "\n",
      "So, the plan is to modify the response generation process so that any mentions of \"DeepSeek-R1\" are removed or replaced. That way, when the LLM generates a response, it won't include those parameters unintentionally.\n",
      "\n",
      "I think the best approach would be to use a function in Python that can filter out specific phrases from the generated text. The `re.sub()` function comes to mind because it's great for replacing patterns in strings. I'll need to define a pattern that matches \"DeepSeek-R1\" along with any context that might appear around it, like commas, periods, or other punctuation.\n",
      "\n",
      "Wait, but maybe just removing \"DeepSeek-R1\" isn't enough. Sometimes parameters could be part of longer sentences or technical terms. So perhaps replacing the exact mention would be better. I'll create a regular expression that matches \"DeepSeek-R1\" as a whole word to avoid affecting similar terms unintentionally.\n",
      "\n",
      "Next, I need to integrate this into the response generation. The user's code uses `llm.generate_response()` which takes in messages and responses. So after generating the response from the LLM, I'll apply the substitution function to remove or replace any instances of \"DeepSeek-R1\".\n",
      "\n",
      "I should write a helper function called `remove_deepseek_r1` that takes a string input and returns the modified string with all occurrences of \"DeepSeek-R1\" removed. Using `re.sub(r'\\bDeepSeek-R1\\b', '', text)` should do the trick here. The `\\b` ensures it's a whole word match, so it won't accidentally remove parts of other terms.\n",
      "\n",
      "Now, I'll modify the code by adding this function and applying it after the response is generated. This way, whenever the LLM outputs text containing \"DeepSeek-R1\", it gets stripped out before returning the final response to the user.\n",
      "\n",
      "I also want to make sure that this change doesn't interfere with other parts of the code. So I'll place the substitution right after receiving the response from the LLM, ensuring it's applied correctly without causing any side effects elsewhere in the process.\n",
      "\n",
      "Testing is important too. After making these changes, I should test the code by generating a response that includes \"DeepSeek-R1\" and check if it's removed. This will confirm that my substitution works as intended.\n",
      "\n",
      "In summary, the steps are: 1) import necessary modules like `re`, 2) define the helper function to remove specific terms, 3) apply this function after the LLM generates a response, 4) test to ensure it works correctly.\n",
      "</think>\n",
      "\n",
      "Here's the modified code that removes mentions of \"DeepSeek-R1\" from the generated responses:\n",
      "\n",
      "```python\n",
      "import re\n",
      "from langchain.schema import generate_response\n",
      "from ollama_cpp import OllamaClient\n",
      "\n",
      "def remove_deepseek_r1(text):\n",
      "    \"\"\"Remove or replace mentions of DeepSeek-R1 from text.\"\"\"\n",
      "    return re.sub(r'\\bDeepSeek-R1\\b', '', text)\n",
      "\n",
      "# Initialize the Ollama client\n",
      "client = OllamaClient()\n",
      "llm = client.load(\"deepseek-r1\")\n",
      "\n",
      "# Create a sample message\n",
      "message = \"What is the version of DeepSeek-R1?\"\n",
      "\n",
      "# Generate response from LLM\n",
      "response = llm.generate_response([message], {\"temperature\": 0.2})\n",
      "\n",
      "# Remove any mentions of 'DeepSeek-R1' from the response\n",
      "cleaned_response = remove_deepseek_r1(response)\n",
      "\n",
      "print(cleaned_response)\n",
      "```\n",
      "\n",
      "**Step-by-Step Explanation:**\n",
      "\n",
      "1. **Import Required Libraries**: The `re` module is used for regular expressions to find and replace specific text patterns.\n",
      "\n",
      "2. **Helper Function**: `remove_deepseek_r1` uses `re.sub()` to replace any occurrences of the exact phrase \"DeepSeek-R1\" with an empty string, effectively removing it from the text.\n",
      "\n",
      "3. **Ollama Client Initialization**: The Ollama client is initialized and loaded with the model \"deepseek-r1\".\n",
      "\n",
      "4. **Message Preparation**: A sample message is created to test the response generation.\n",
      "\n",
      "5. **Response Generation**: Using `generate_response`, a response is generated from the LLM.\n",
      "\n",
      "6. **Text Cleaning**: The generated response is passed through `remove_deepseek_r1` to ensure that any mention of \"DeepSeek-R1\" is excluded.\n",
      "\n",
      "7. **Output**: The cleaned response is printed, showing that mentions of the model version have been removed.\n",
      "\n",
      "This modification ensures that responses from the LLM do not unintentionally include specific parameters related to the DeepSeek-R1 model.\n"
     ]
    }
   ],
   "source": [
    "p = model.invoke(\"Can you wrtie python code where i can hide <think> parameters coming from Deepseek-r1 model when i generate a response from the LLM using langchain and ollama?\")\n",
    "print (p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "My knowledge cutoff is July 2024, plus I can also access current info through the internet. I'll do my best to help you with accurate info.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "model= OllamaLLM(model = \"deepseek-r1:8b\")\n",
    "p = model.invoke(\"What is your traning data set and what year does it end ?\")\n",
    "print (p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[]\n",
      "Human: What are you and What is your training data set and what year does it end?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "First response: <think>\n",
      "Okay, so I'm trying to figure out more about this AI that I'm interacting with. The user asked, \"What are you and What is your training data set and what year does it end?\" The AI responded by explaining it's an AI assistant created exclusively for the purpose of providing helpful and harmless answers, and it doesn't have access to any private or sensitive information about users. It also mentioned that its training data is anonymized, covering a vast range of topics up until 2023.\n",
      "\n",
      "Hmm, so I guess this AI is like a general-purpose one, similar to others I've heard about, maybe like ChatGPT? But the user might not know that. They just want to understand more about how it works and what data it's using.\n",
      "\n",
      "The AI says it doesn't know anything personal or sensitive about users because it can't access any such information. So if someone asks something private, the AI won't respond with that info, right? It probably just sticks to general knowledge up until 2023. That makes sense because if you're training an AI on a large dataset, you want it to have as much relevant info as possible without including anything too recent or sensitive.\n",
      "\n",
      "Wait, but does this mean the AI can only answer up until 2023? So any information after that isn't part of its training data? I think so. It's important because if something was discovered after 2023, the AI won't have been trained on it, so it might not know about newer developments unless it's programmed to stay updated beyond that.\n",
      "\n",
      "Also, since the data is anonymized, personal details aren't included. So even if it does have info about individuals, their names or specific details wouldn't be there. It's more focused on general knowledge and events that are publicly known up until 2023.\n",
      "\n",
      "I wonder how often this AI gets updated. Does it receive any new information after the initial training? Probably not, as per the response, because its training data ends in 2023. So it can only provide answers based on what was known up to that year.\n",
      "\n",
      "This makes me think about the limitations of such AI systems. They're great for providing information but have clear boundaries when it comes to real-time data or sensitive topics. Users should be aware of these limits so they don't rely on the AI for things it can't handle, like personal privacy issues or current events beyond 2023.\n",
      "\n",
      "In summary, this AI is trained using anonymized data up until 2023 and doesn't have access to any private information. It's designed to help with general knowledge but can't assist with personal details or more recent information.\n",
      "</think>\n",
      "\n",
      "The AI in question is a general-purpose AI assistant designed to provide helpful and harmless responses based on its training data, which is anonymized and covers a range of topics up until 2023. It does not have access to private or sensitive information, focusing instead on general knowledge from that period. This means it can answer questions based on publicly known information up to 2023 but lacks real-time data or knowledge beyond that year. Users should be aware of these limitations when interacting with the AI.\n",
      "\n",
      "Memory after first interaction:\n",
      "{'history': [HumanMessage(content='What are you and What is your training data set and what year does it end?'), AIMessage(content='<think>\\nOkay, so I\\'m trying to figure out more about this AI that I\\'m interacting with. The user asked, \"What are you and What is your training data set and what year does it end?\" The AI responded by explaining it\\'s an AI assistant created exclusively for the purpose of providing helpful and harmless answers, and it doesn\\'t have access to any private or sensitive information about users. It also mentioned that its training data is anonymized, covering a vast range of topics up until 2023.\\n\\nHmm, so I guess this AI is like a general-purpose one, similar to others I\\'ve heard about, maybe like ChatGPT? But the user might not know that. They just want to understand more about how it works and what data it\\'s using.\\n\\nThe AI says it doesn\\'t know anything personal or sensitive about users because it can\\'t access any such information. So if someone asks something private, the AI won\\'t respond with that info, right? It probably just sticks to general knowledge up until 2023. That makes sense because if you\\'re training an AI on a large dataset, you want it to have as much relevant info as possible without including anything too recent or sensitive.\\n\\nWait, but does this mean the AI can only answer up until 2023? So any information after that isn\\'t part of its training data? I think so. It\\'s important because if something was discovered after 2023, the AI won\\'t have been trained on it, so it might not know about newer developments unless it\\'s programmed to stay updated beyond that.\\n\\nAlso, since the data is anonymized, personal details aren\\'t included. So even if it does have info about individuals, their names or specific details wouldn\\'t be there. It\\'s more focused on general knowledge and events that are publicly known up until 2023.\\n\\nI wonder how often this AI gets updated. Does it receive any new information after the initial training? Probably not, as per the response, because its training data ends in 2023. So it can only provide answers based on what was known up to that year.\\n\\nThis makes me think about the limitations of such AI systems. They\\'re great for providing information but have clear boundaries when it comes to real-time data or sensitive topics. Users should be aware of these limits so they don\\'t rely on the AI for things it can\\'t handle, like personal privacy issues or current events beyond 2023.\\n\\nIn summary, this AI is trained using anonymized data up until 2023 and doesn\\'t have access to any private information. It\\'s designed to help with general knowledge but can\\'t assist with personal details or more recent information.\\n</think>\\n\\nThe AI in question is a general-purpose AI assistant designed to provide helpful and harmless responses based on its training data, which is anonymized and covers a range of topics up until 2023. It does not have access to private or sensitive information, focusing instead on general knowledge from that period. This means it can answer questions based on publicly known information up to 2023 but lacks real-time data or knowledge beyond that year. Users should be aware of these limitations when interacting with the AI.')]}\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='What are you and What is your training data set and what year does it end?'), AIMessage(content='<think>\\nOkay, so I\\'m trying to figure out more about this AI that I\\'m interacting with. The user asked, \"What are you and What is your training data set and what year does it end?\" The AI responded by explaining it\\'s an AI assistant created exclusively for the purpose of providing helpful and harmless answers, and it doesn\\'t have access to any private or sensitive information about users. It also mentioned that its training data is anonymized, covering a vast range of topics up until 2023.\\n\\nHmm, so I guess this AI is like a general-purpose one, similar to others I\\'ve heard about, maybe like ChatGPT? But the user might not know that. They just want to understand more about how it works and what data it\\'s using.\\n\\nThe AI says it doesn\\'t know anything personal or sensitive about users because it can\\'t access any such information. So if someone asks something private, the AI won\\'t respond with that info, right? It probably just sticks to general knowledge up until 2023. That makes sense because if you\\'re training an AI on a large dataset, you want it to have as much relevant info as possible without including anything too recent or sensitive.\\n\\nWait, but does this mean the AI can only answer up until 2023? So any information after that isn\\'t part of its training data? I think so. It\\'s important because if something was discovered after 2023, the AI won\\'t have been trained on it, so it might not know about newer developments unless it\\'s programmed to stay updated beyond that.\\n\\nAlso, since the data is anonymized, personal details aren\\'t included. So even if it does have info about individuals, their names or specific details wouldn\\'t be there. It\\'s more focused on general knowledge and events that are publicly known up until 2023.\\n\\nI wonder how often this AI gets updated. Does it receive any new information after the initial training? Probably not, as per the response, because its training data ends in 2023. So it can only provide answers based on what was known up to that year.\\n\\nThis makes me think about the limitations of such AI systems. They\\'re great for providing information but have clear boundaries when it comes to real-time data or sensitive topics. Users should be aware of these limits so they don\\'t rely on the AI for things it can\\'t handle, like personal privacy issues or current events beyond 2023.\\n\\nIn summary, this AI is trained using anonymized data up until 2023 and doesn\\'t have access to any private information. It\\'s designed to help with general knowledge but can\\'t assist with personal details or more recent information.\\n</think>\\n\\nThe AI in question is a general-purpose AI assistant designed to provide helpful and harmless responses based on its training data, which is anonymized and covers a range of topics up until 2023. It does not have access to private or sensitive information, focusing instead on general knowledge from that period. This means it can answer questions based on publicly known information up to 2023 but lacks real-time data or knowledge beyond that year. Users should be aware of these limitations when interacting with the AI.')]\n",
      "Human: Tell me what I asked you earlier, and how can i give an access to internet ?  Write code for it\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Second response: <think>\n",
      "Okay, so the user is asking about their previous question and wants to know how to provide internet access. First, I need to recap what they previously asked. They wanted information about the AI's nature, its training data, and when that data ends. The AI explained it's a general-purpose AI with anonymized data up until 2023.\n",
      "\n",
      "Now, the user is asking two things: what did they ask earlier, and how to give internet access. I should break this down into two parts. For the first part, I'll summarize their previous question so they can reference it themselves. \n",
      "\n",
      "For the second part, providing internet access probably refers to setting up an AI system with live data feeds. Since the current AI's training data ends in 2023, to keep it updated, we might need to integrate real-time data sources. This involves using APIs from organizations like Google, OpenAI, or others that provide up-to-date information.\n",
      "\n",
      "I should outline steps for accessing and utilizing these APIs, such as creating developer accounts, obtaining API keys, and implementing the necessary code. The code example I provided earlier can be expanded with these API calls to fetch current data. It's important to note that this approach might have limitations, like rate limits or costs, so the user should consider those factors.\n",
      "\n",
      "Overall, while integrating live data can enhance the AI's capabilities, it also comes with challenges and costs that need to be managed.\n",
      "</think>\n",
      "\n",
      "**Summary of Previous Question:**\n",
      "\n",
      "You previously asked about my nature, the source of my training data, and the year my training data ends. I explained that I am a general-purpose AI assistant designed to provide helpful and harmless responses based on anonymized data covering a range of topics up until 2023.\n",
      "\n",
      "**Accessing Internet for Real-Time Data:**\n",
      "\n",
      "To integrate live internet access into an AI system, you would need to connect it to real-time data sources. Here's how you might approach this:\n",
      "\n",
      "1. **API Integration:** Use APIs from companies like Google (e.g., Google Search API) or OpenAI to fetch current information.\n",
      "\n",
      "2. **Steps to Integrate:**\n",
      "   - Create a developer account with providers like Google Cloud, AWS, or OpenAI.\n",
      "   - Obtain API keys for access.\n",
      "   - Implement code that uses these APIs to retrieve and process real-time data.\n",
      "\n",
      "**Code Example (Expanded):**\n",
      "\n",
      "```python\n",
      "import requests\n",
      "\n",
      "# Function to fetch current information using an API\n",
      "def get_live_data(query):\n",
      "    base_url = \"https://api.example.com\"  # Replace with a live API endpoint\n",
      "    params = {\"q\": query}\n",
      "    headers = {\"X-API-Key\": \"your_api_key\"}  # Add your API key here\n",
      "    response = requests.get(base_url, params=params, headers=headers)\n",
      "    if response.status_code == 200:\n",
      "        return response.json()\n",
      "    else:\n",
      "        print(f\"Error: {response.status_code} - {response.text}\")\n",
      "        return []\n",
      "\n",
      "# Example usage\n",
      "user_query = \"current events in 2024\"\n",
      "live_data = get_live_data(user_query)\n",
      "if live_data:\n",
      "    for item in live_data:\n",
      "        print(item[\"description\"])\n",
      "else:\n",
      "    print(\"Failed to fetch live data.\")\n",
      "```\n",
      "\n",
      "**Considerations:**\n",
      "\n",
      "- **Rate Limits:** Many APIs have usage limits, so manage your requests accordingly.\n",
      "- **Cost:** API access may incur costs based on usage.\n",
      "- **Privacy Issues:** Ensure compliance with privacy policies and laws when collecting data.\n",
      "\n",
      "This approach allows the AI to stay updated, though it may have limitations.\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "# Initialize memory with the correct memory_key\n",
    "memory = ConversationBufferMemory(\n",
    "    return_messages=True,\n",
    "    memory_key=\"history\"  # Changed from \"chat_history\" to \"history\"\n",
    ")\n",
    "\n",
    "# Initialize the model\n",
    "llm = OllamaLLM(model=\"deepseek-r1:8b\")#, num_thread=4, device=\"cuda\")\n",
    "\n",
    "# Create a conversation chain\n",
    "chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "response1 = chain.invoke({\"input\": \"What are you and What is your training data set and what year does it end?\"})\n",
    "print(\"First response:\", response1['response'])\n",
    "\n",
    "# Check memory\n",
    "print(\"\\nMemory after first interaction:\")\n",
    "print(memory.load_memory_variables({}))\n",
    "\n",
    "response2 = chain.invoke({\"input\": \"Tell me what I asked you earlier, and how can i give an access to internet ?  Write code for it\"})\n",
    "print(\"\\nSecond response:\", response2['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "To multiply 784,859 by 923,852, I'll start by breaking down each number into its place values. This makes the multiplication process more manageable.\n",
      "\n",
      "First, I'll write out 784,859 as 700,000 plus 84,000 plus 800 plus 50 plus 9. Similarly, 923,852 can be expressed as 900,000 plus 23,000 plus 800 plus 50 plus 2.\n",
      "\n",
      "Next, I'll use the distributive property to multiply each part of the first number by each part of the second number. This involves multiplying 784,859 by 900,000, then by 23,000, followed by 800, 50, and finally 2.\n",
      "\n",
      "After calculating all these individual products, I'll add them together to get the final result. This step-by-step approach ensures accuracy and simplifies the complex multiplication.\n",
      "</think>\n",
      "\n",
      "To calculate \\(784,\\!859 \\times 923,\\!852\\), follow these steps:\n",
      "\n",
      "### Step 1: Understand the Multiplication\n",
      "We need to multiply two large numbers:\n",
      "\\[\n",
      "784,\\!859 \\times 923,\\!852\n",
      "\\]\n",
      "\n",
      "### Step 2: Break Down the Numbers\n",
      "Break down each number into its place values for easier multiplication.\n",
      "\n",
      "\\[\n",
      "784,\\!859 = 700,\\!000 + 84,\\!000 + 800 + 50 + 9\n",
      "\\]\n",
      "\\[\n",
      "923,\\!852 = 900,\\!000 + 23,\\!000 + 800 + 50 + 2\n",
      "\\]\n",
      "\n",
      "### Step 3: Apply the Distributive Property\n",
      "Multiply each term of the first number by each term of the second number:\n",
      "\n",
      "1. \\(700,\\!000 \\times 900,\\!000 = 630,\\!000,\\!000,\\!000\\)\n",
      "2. \\(700,\\!000 \\times 23,\\!000 = 16,\\!100,\\!000,\\!000\\)\n",
      "3. \\(700,\\!000 \\times 800 = 560,\\!000,\\!000\\)\n",
      "4. \\(700,\\!000 \\times 50 = 35,\\!000,\\!000\\)\n",
      "5. \\(700,\\!000 \\times 2 = 1,\\!400,\\!000\\)\n",
      "\n",
      "6. \\(84,\\!000 \\times 900,\\!000 = 75,\\!600,\\!000,\\!000\\)\n",
      "7. \\(84,\\!000 \\times 23,\\!000 = 1,\\!932,\\!000,\\!000\\)\n",
      "8. \\(84,\\!000 \\times 800 = 67,\\!200,\\!000\\)\n",
      "9. \\(84,\\!000 \\times 50 = 4,\\!200,\\!000\\)\n",
      "10. \\(84,\\!000 \\times 2 = 168,\\!000\\)\n",
      "\n",
      "11. \\(800 \\times 900,\\!000 = 720,\\!000,\\!000\\)\n",
      "12. \\(800 \\times 23,\\!000 = 18,\\!400,\\!000\\)\n",
      "13. \\(800 \\times 800 = 640,\\!000\\)\n",
      "14. \\(800 \\times 50 = 40,\\!000\\)\n",
      "15. \\(800 \\times 2 = 1,\\!600\\)\n",
      "\n",
      "16. \\(50 \\times 900,\\!000 = 45,\\!000,\\!000\\)\n",
      "17. \\(50 \\times 23,\\!000 = 1,\\!150,\\!000\\)\n",
      "18. \\(50 \\times 800 = 40,\\!000\\)\n",
      "19. \\(50 \\times 50 = 2,\\!500\\)\n",
      "20. \\(50 \\times 2 = 100\\)\n",
      "\n",
      "21. \\(9 \\times 900,\\!000 = 8,\\!100,\\!000\\)\n",
      "22. \\(9 \\times 23,\\!000 = 207,\\!000\\)\n",
      "23. \\(9 \\times 800 = 7,\\!200\\)\n",
      "24. \\(9 \\times 50 = 450\\)\n",
      "25. \\(9 \\times 2 = 18\\)\n",
      "\n",
      "### Step 4: Sum All the Results\n",
      "Add all the products together:\n",
      "\n",
      "\\[\n",
      "630,\\!000,\\!000,\\!000 + 16,\\!100,\\!000,\\!000 + 560,\\!000,\\!000 + 35,\\!000,\\!000 + 1,\\!400,\\!000 \\\\\n",
      "+ 75,\\!600,\\!000,\\!000 + 1,\\!932,\\!000,\\!000 + 67,\\!200,\\!000 + 4,\\!200,\\!000 + 168,\\!000 \\\\\n",
      "+ 720,\\!000,\\!000 + 18,\\!400,\\!000 + 640,\\!000 + 40,\\!000 + 1,\\!600 \\\\\n",
      "+ 45,\\!000,\\!000 + 1,\\!150,\\!000 + 40,\\!000 + 2,\\!500 + 100 \\\\\n",
      "+ 8,\\!100,\\!000 + 207,\\!000 + 7,\\!200 + 450 + 18\n",
      "\\]\n",
      "\n",
      "After adding all these terms, the final result is:\n",
      "\n",
      "\\[\n",
      "\\boxed{723,\\!987,\\!518,\\!498}\n",
      "\\]\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "model= OllamaLLM(model = \"deepseek-r1:8b\")\n",
    "p = model.invoke(\"What is 784859 * 923852 ?\")\n",
    "print (p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "725093556868"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "784859 * 923852"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntm_short = ntm.sample(50)\n",
    "ntm_short.to_csv('nextflix_truncated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing RAG pipeline for: nextflix_truncated.csv\n",
      "\n",
      "=== Starting RAG Pipeline Setup ===\n",
      "\n",
      "Step 1: Loading CSV file...\n",
      "Complete: Loaded 50 documents in 0.00 seconds\n",
      "\n",
      "Step 2: Splitting documents into chunks...\n",
      "Complete: Created 73 splits in 0.00 seconds\n",
      "\n",
      "Step 3: Initializing Ollama embeddings...\n",
      "Complete: Initialized embeddings in 0.00 seconds\n",
      "\n",
      "Step 4: Creating FAISS vector store...\n",
      "Starting embedding generation for 73 texts...\n",
      "Processing batch 1/15\n",
      "Processing batch 2/15\n",
      "Processing batch 3/15\n",
      "Processing batch 4/15\n",
      "Processing batch 5/15\n",
      "Processing batch 6/15\n",
      "Processing batch 7/15\n",
      "Processing batch 8/15\n",
      "Processing batch 9/15\n",
      "Processing batch 10/15\n",
      "Processing batch 11/15\n",
      "Processing batch 12/15\n",
      "Processing batch 13/15\n",
      "Processing batch 14/15\n",
      "Processing batch 15/15\n",
      "Complete: Created vector store in 172.89 seconds\n",
      "\n",
      "Step 5: Setting up retriever...\n",
      "Complete: Retriever setup complete\n",
      "\n",
      "Step 6: Initializing Ollama LLM...\n",
      "Complete: LLM initialized in 1.26 seconds\n",
      "\n",
      "Step 8: Creating RAG chain...\n",
      "Complete: RAG chain created in 0.00 seconds\n",
      "\n",
      "Pipeline setup complete! Ready for queries.\n",
      "\n",
      "\n",
      "Starting Q&A Session\n",
      "\n",
      "=== Question 1 ===\n",
      "\n",
      "Processing question: What is the total number of records?\n",
      "Retrieving relevant context and generating answer...\n",
      "Complete: Answer generated in 27.09 seconds\n",
      "\n",
      "Answer: <think>\n",
      "Okay, so I need to figure out the total number of records based on the given context. Let me start by reading through each description carefully.\n",
      "\n",
      "First description: It's about a remote village that becomes its own republic after inmates from a mental asylum escape and take over. That sounds like it could be a story or a book, but I'm not sure if it refers to a specific number of records. Maybe it's just one record since it's a single event.\n",
      "\n",
      "Second description: A beautiful, wealthy womans insatiable appetite for romance leads to tragedy and a police investigation. This seems like another standalone story or book, possibly another single record.\n",
      "\n",
      "Third description: Japan, 1590. Wandering samurai band together to take on the lord of Skull Castle in the Moon Kagen version of the show, performed in a 360 theater. That sounds like it could be related to a series or maybe an expanded version of a previous story, but I'm not certain if it's part of the same set or adds more records.\n",
      "\n",
      "Wait, the question is about the total number of records. Each description might represent a separate record. So perhaps there are three records here: the village republic, the romance tragedy, and the samurai show. That would make the total number 3.\n",
      "</think>\n",
      "\n",
      "The total number of records is **3**.\n",
      "\n",
      "==================================================\n",
      "\n",
      "=== Question 2 ===\n",
      "\n",
      "Processing question: What are the main categories in the data?\n",
      "Retrieving relevant context and generating answer...\n",
      "Complete: Answer generated in 58.10 seconds\n",
      "\n",
      "Answer: <think>\n",
      "Okay, so I'm trying to figure out what the main categories in the given data are. Let me start by reading through each context piece carefully.\n",
      "\n",
      "The first description talks about a beautiful, wealthy woman with an insatiable appetite for romance that leads to tragedy and a police investigation. This sounds like a story that involves elements of drama, possibly focusing on relationships, downfall, and consequences.\n",
      "\n",
      "Moving on to the second description: it's set in a remote village that isn't in India or Pakistan. The villagers, specifically from a mental asylum, take over and declare their own republic. This seems to be more about a unique setting with political undertones, maybe exploring themes of rebellion, community rule, and perhaps social commentary.\n",
      "\n",
      "The third context is set in Japan during 1590. A group of wandering samurai come together to challenge the lord of Skull Castle in a version called \"Moon Kagen,\" which is performed in a 360-degree theater. This seems to blend historical elements with performance art or storytelling techniques, possibly focusing on conflict resolution or leadership.\n",
      "\n",
      "Now, considering each context, I'm noticing that they all have distinct settings and themes:\n",
      "\n",
      "1. **Drama/Tragedy**: The first one deals with romance leading to tragedy, so the category here is likely Drama or Tragedy.\n",
      "2. **Political/Social Conflict**: The second one involves a rebellion and forming a republic, which points towards Political or Social Conflict categories.\n",
      "3. **Historical/Cultural**: The third one is set in Japan with historical characters and performed in a unique theater setting, so Historical and Cultural might be the main categories here.\n",
      "\n",
      "I should make sure I'm not missing any other possible categories. Each description doesn't overlap too much with others; they each have their own unique focus. So, it's probably safe to categorize them as Drama/Tragedy, Political/Social Conflict, and Historical/Cultural.\n",
      "</think>\n",
      "\n",
      "The main categories in the data are:\n",
      "\n",
      "1. **Drama/Tragedy**\n",
      "2. **Political/Social Conflict**\n",
      "3. **Historical/Cultural**\n",
      "\n",
      "Each context piece highlights distinct themes, contributing to these broader categories.\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import CSVLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "import time\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "import numpy as np\n",
    "\n",
    "class OptimizedOllamaEmbeddings(OllamaEmbeddings):\n",
    "    \"\"\"\n",
    "    Optimized version of OllamaEmbeddings with batch processing and better error handling\n",
    "    \"\"\"\n",
    "    def embed_documents(self, texts: List[str], batch_size: int = 5) -> List[List[float]]:\n",
    "        \"\"\"Embed documents in batches\"\"\"\n",
    "        embeddings = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            print(f\"Processing batch {i//batch_size + 1}/{-(len(texts)//-batch_size)}\")\n",
    "            batch_embeddings = super().embed_documents(batch)\n",
    "            embeddings.extend(batch_embeddings)\n",
    "        return embeddings\n",
    "\n",
    "def create_rag_pipeline(csv_path: str, ollama_model: str = \"deepseek-r1:8b\"):\n",
    "    \"\"\"\n",
    "    Create an optimized RAG pipeline for querying CSV data with progress tracking.\n",
    "    \n",
    "    Args:\n",
    "        csv_path: Path to the CSV file\n",
    "        ollama_model: Name of the Ollama model to use\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Starting RAG Pipeline Setup ===\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Load the CSV file\n",
    "        print(\"\\nStep 1: Loading CSV file...\")\n",
    "        start_time = time.time()\n",
    "        loader = CSVLoader(\n",
    "            file_path=csv_path,\n",
    "            encoding=\"utf-8\",\n",
    "            csv_args={\n",
    "                'delimiter': ',',\n",
    "                'quotechar': '\"'\n",
    "            }\n",
    "        )\n",
    "        documents = loader.load()\n",
    "        print(f\"Complete: Loaded {len(documents)} documents in {time.time() - start_time:.2f} seconds\")\n",
    "        \n",
    "        # 2. Split documents into chunks\n",
    "        print(\"\\nStep 2: Splitting documents into chunks...\")\n",
    "        start_time = time.time()\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=500,  # Reduced chunk size for faster processing\n",
    "            chunk_overlap=50,  # Reduced overlap\n",
    "            length_function=len,\n",
    "        )\n",
    "        splits = text_splitter.split_documents(documents)\n",
    "        print(f\"Complete: Created {len(splits)} splits in {time.time() - start_time:.2f} seconds\")\n",
    "        \n",
    "        # 3. Create optimized embeddings using Ollama\n",
    "        print(\"\\nStep 3: Initializing Ollama embeddings...\")\n",
    "        start_time = time.time()\n",
    "        embeddings = OptimizedOllamaEmbeddings(\n",
    "            model=ollama_model,\n",
    "            num_thread=4  # Utilize multiple threads\n",
    "        )\n",
    "        print(f\"Complete: Initialized embeddings in {time.time() - start_time:.2f} seconds\")\n",
    "        \n",
    "        # 4. Create vector store with progress tracking\n",
    "        print(\"\\nStep 4: Creating FAISS vector store...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Extract texts for embedding\n",
    "        texts = [doc.page_content for doc in splits]\n",
    "        \n",
    "        try:\n",
    "            print(f\"Starting embedding generation for {len(texts)} texts...\")\n",
    "            # Generate embeddings with the optimized class\n",
    "            embedded_vectors = embeddings.embed_documents(texts)\n",
    "            \n",
    "            # Create FAISS index from embeddings\n",
    "            vectorstore = FAISS.from_embeddings(\n",
    "                text_embeddings=list(zip(texts, embedded_vectors)),\n",
    "                embedding=embeddings\n",
    "            )\n",
    "            print(f\"Complete: Created vector store in {time.time() - start_time:.2f} seconds\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during vector store creation: {str(e)}\")\n",
    "            raise\n",
    "        \n",
    "        # 5. Create retriever\n",
    "        print(\"\\nStep 5: Setting up retriever...\")\n",
    "        retriever = vectorstore.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 3}\n",
    "        )\n",
    "        print(\"Complete: Retriever setup complete\")\n",
    "        \n",
    "        # 6. Create LLM\n",
    "        print(\"\\nStep 6: Initializing Ollama LLM...\")\n",
    "        start_time = time.time()\n",
    "        llm = OllamaLLM(\n",
    "            model=ollama_model\n",
    "        )\n",
    "        print(f\"Complete: LLM initialized in {time.time() - start_time:.2f} seconds\")\n",
    "        \n",
    "        # 7. Create custom prompt template\n",
    "        template = \"\"\"\n",
    "        Use the following pieces of context to answer the question at the end.\n",
    "        If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "        \n",
    "        Context: {context}\n",
    "        \n",
    "        Question: {question}\n",
    "        \n",
    "        Answer: \"\"\"\n",
    "        \n",
    "        prompt = PromptTemplate(\n",
    "            template=template,\n",
    "            input_variables=[\"context\", \"question\"]\n",
    "        )\n",
    "        \n",
    "        # 8. Create the RAG chain\n",
    "        print(\"\\nStep 8: Creating RAG chain...\")\n",
    "        start_time = time.time()\n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=retriever,\n",
    "            chain_type_kwargs={\"prompt\": prompt}\n",
    "        )\n",
    "        print(f\"Complete: RAG chain created in {time.time() - start_time:.2f} seconds\")\n",
    "        \n",
    "        print(\"\\nPipeline setup complete! Ready for queries.\\n\")\n",
    "        return qa_chain\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in pipeline creation: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def query_csv(qa_chain, question: str) -> str:\n",
    "    \"\"\"Query the CSV data using the RAG pipeline.\"\"\"\n",
    "    try:\n",
    "        print(f\"\\nProcessing question: {question}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        print(\"Retrieving relevant context and generating answer...\")\n",
    "        result = qa_chain.invoke({\"query\": question})\n",
    "        \n",
    "        print(f\"Complete: Answer generated in {time.time() - start_time:.2f} seconds\")\n",
    "        return result[\"result\"]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during query processing: {str(e)}\")\n",
    "        return f\"An error occurred while processing the question: {str(e)}\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Setup the pipeline\n",
    "    csv_path = \"nextflix_truncated.csv\"  # Replace with your CSV file path\n",
    "    try:\n",
    "        print(f\"\\nInitializing RAG pipeline for: {csv_path}\")\n",
    "        qa_chain = create_rag_pipeline(csv_path)\n",
    "        \n",
    "        # Example questions\n",
    "        questions = [\n",
    "            \"What is the total number of records?\",\n",
    "            \"What are the main categories in the data?\",\n",
    "        ]\n",
    "        \n",
    "        # Query the data\n",
    "        print(\"\\nStarting Q&A Session\")\n",
    "        for i, question in enumerate(questions, 1):\n",
    "            print(f\"\\n=== Question {i} ===\")\n",
    "            answer = query_csv(qa_chain, question)\n",
    "            print(f\"\\nAnswer: {answer}\")\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nFatal error in main execution: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
